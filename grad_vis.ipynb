{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import cv2\n",
    "\n",
    "from mdistiller.models import cifar_model_dict\n",
    "from mdistiller.dataset import get_dataset\n",
    "from mdistiller.engine.utils import load_checkpoint\n",
    "from mdistiller.engine.cfg import CFG as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_metric(model, val_loader, num_classes=100):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in tqdm(enumerate(val_loader)):\n",
    "            outputs, _ = model(data)\n",
    "            preds = outputs\n",
    "            all_preds.append(preds.data.cpu().numpy())\n",
    "            all_labels.append(labels.data.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, 0)\n",
    "    all_labels = np.concatenate(all_labels, 0)\n",
    "    matrix = np.zeros((num_classes, num_classes))\n",
    "    cnt = np.zeros((num_classes, 1))\n",
    "    for p, l in zip(all_preds, all_labels):\n",
    "        cnt[l, 0] += 1\n",
    "        matrix[l] += p\n",
    "    matrix /= cnt\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsAndGradients:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targeted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers, reshape_transform):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.handles = []\n",
    "        for target_layer in target_layers:\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(\n",
    "                    self.save_activation))\n",
    "            # Backward compatibility with older pytorch versions:\n",
    "            if hasattr(target_layer, 'register_full_backward_hook'):\n",
    "                self.handles.append(\n",
    "                    target_layer.register_full_backward_hook(\n",
    "                        self.save_gradient))\n",
    "            else:\n",
    "                self.handles.append(\n",
    "                    target_layer.register_backward_hook(\n",
    "                        self.save_gradient))\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        activation = output\n",
    "        if self.reshape_transform is not None:\n",
    "            activation = self.reshape_transform(activation)\n",
    "        self.activations.append(activation.cpu().detach())\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        # Gradients are computed in reverse order\n",
    "        grad = grad_output[0]\n",
    "        if self.reshape_transform is not None:\n",
    "            grad = self.reshape_transform(grad)\n",
    "        self.gradients = [grad.cpu().detach()] + self.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        return self.model(x)\n",
    "\n",
    "    def release(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 target_layers,\n",
    "                 reshape_transform=None,\n",
    "                 use_cuda=False):\n",
    "        self.model = model.eval()\n",
    "        self.target_layers = target_layers\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "        self.activations_and_grads = ActivationsAndGradients(\n",
    "            self.model, target_layers, reshape_transform)\n",
    "\n",
    "    \"\"\" Get a vector of weights for every channel in the target layer.\n",
    "        Methods that return weights channels,\n",
    "        will typically need to only implement this function. \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cam_weights(grads):\n",
    "        return np.mean(grads, axis=(2, 3), keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss(output, target_category):\n",
    "        loss = 0\n",
    "        for i in range(len(target_category)):\n",
    "            loss = loss + output[i, target_category[i]]\n",
    "        return loss\n",
    "\n",
    "    def get_cam_image(self, activations, grads):\n",
    "        weights = self.get_cam_weights(grads)\n",
    "        weighted_activations = weights * activations\n",
    "        cam = weighted_activations.sum(axis=1)\n",
    "\n",
    "        return cam\n",
    "\n",
    "    @staticmethod\n",
    "    def get_target_width_height(input_tensor):\n",
    "        width, height = input_tensor.size(-1), input_tensor.size(-2)\n",
    "        return width, height\n",
    "\n",
    "    def compute_cam_per_layer(self, input_tensor):\n",
    "        activations_list = [a.cpu().data.numpy()\n",
    "                            for a in self.activations_and_grads.activations]\n",
    "        grads_list = [g.cpu().data.numpy()\n",
    "                      for g in self.activations_and_grads.gradients]\n",
    "        target_size = self.get_target_width_height(input_tensor)\n",
    "\n",
    "        cam_per_target_layer = []\n",
    "        # Loop over the saliency image from every layer\n",
    "\n",
    "        for layer_activations, layer_grads in zip(activations_list, grads_list):\n",
    "            cam = self.get_cam_image(layer_activations, layer_grads)\n",
    "            cam[cam < 0] = 0  # works like mute the min-max scale in the function of scale_cam_image\n",
    "            scaled = self.scale_cam_image(cam, target_size)\n",
    "            cam_per_target_layer.append(scaled[:, None, :])\n",
    "\n",
    "        return cam_per_target_layer\n",
    "\n",
    "    def aggregate_multi_layers(self, cam_per_target_layer):\n",
    "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
    "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
    "        result = np.mean(cam_per_target_layer, axis=1)\n",
    "        return self.scale_cam_image(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_cam_image(cam, target_size=None):\n",
    "        result = []\n",
    "        for img in cam:\n",
    "            img = img - np.min(img)\n",
    "            img = img / (1e-7 + np.max(img))\n",
    "            if target_size is not None:\n",
    "                img = cv2.resize(img, target_size)\n",
    "            result.append(img)\n",
    "        result = np.float32(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __call__(self, input_tensor, target_category=None):\n",
    "\n",
    "        if self.cuda:\n",
    "            input_tensor = input_tensor.cuda()\n",
    "\n",
    "        # 正向传播得到网络输出logits(未经过softmax)\n",
    "        output = self.activations_and_grads(input_tensor)\n",
    "        if isinstance(target_category, int):\n",
    "            target_category = [target_category] * input_tensor.size(0)\n",
    "\n",
    "        if target_category is None:\n",
    "            target_category = np.argmax(output.cpu().data.numpy(), axis=-1)\n",
    "            print(f\"category id: {target_category}\")\n",
    "        else:\n",
    "            assert (len(target_category) == input_tensor.size(0))\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        loss = self.get_loss(output, target_category)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # In most of the saliency attribution papers, the saliency is\n",
    "        # computed with a single target layer.\n",
    "        # Commonly it is the last convolutional layer.\n",
    "        # Here we support passing a list with multiple target layers.\n",
    "        # It will compute the saliency image for every image,\n",
    "        # and then aggregate them (with a default mean aggregation).\n",
    "        # This gives you more flexibility in case you just want to\n",
    "        # use all conv layers for example, all Batchnorm layers,\n",
    "        # or something else.\n",
    "        cam_per_layer = self.compute_cam_per_layer(input_tensor)\n",
    "        return self.aggregate_multi_layers(cam_per_layer)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.activations_and_grads.release()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.activations_and_grads.release()\n",
    "        if isinstance(exc_value, IndexError):\n",
    "            # Handle IndexError here...\n",
    "            print(\n",
    "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
    "            return True\n",
    "\n",
    "\n",
    "def show_cam_on_image(img: np.ndarray,\n",
    "                      mask: np.ndarray,\n",
    "                      use_rgb: bool = False,\n",
    "                      colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
    "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
    "    By default the heatmap is in BGR format.\n",
    "\n",
    "    :param img: The base image in RGB or BGR format.\n",
    "    :param mask: The cam mask.\n",
    "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
    "    :param colormap: The OpenCV colormap to be used.\n",
    "    :returns: The default image with the cam overlay.\n",
    "    \"\"\"\n",
    "\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    if use_rgb:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    if np.max(img) > 1:\n",
    "        raise Exception(\n",
    "            \"The input image should np.float32 in the range [0, 1]\")\n",
    "\n",
    "    cam = heatmap + img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "\n",
    "def center_crop_img(img: np.ndarray, size: int):\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    if w == h == size:\n",
    "        return img\n",
    "\n",
    "    if w < h:\n",
    "        ratio = size / w\n",
    "        new_w = size\n",
    "        new_h = int(h * ratio)\n",
    "    else:\n",
    "        ratio = size / h\n",
    "        new_h = size\n",
    "        new_w = int(w * ratio)\n",
    "\n",
    "    img = cv2.resize(img, dsize=(new_w, new_h))\n",
    "\n",
    "    if new_w == size:\n",
    "        h = (new_h - size) // 2\n",
    "        img = img[h: h+size]\n",
    "    else:\n",
    "        w = (new_w - size) // 2\n",
    "        img = img[:, w: w+size]\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def load_model(tea, stu, mpath):\n",
    "    cfg.defrost()\n",
    "    cfg.DISTILLER.STUDENT = stu\n",
    "    cfg.DISTILLER.TEACHER = tea\n",
    "    cfg.DATASET.TYPE = 'cifar100'\n",
    "    cfg.freeze()\n",
    "    train_loader, val_loader, num_data, num_classes = get_dataset(cfg)\n",
    "    model = cifar_model_dict[cfg.DISTILLER.STUDENT][0](num_classes=num_classes)\n",
    "    fully_state = load_checkpoint(mpath)[\"model\"]\n",
    "    student_weights = OrderedDict()\n",
    "    teacher_weights = OrderedDict()\n",
    "\n",
    "    for key, value in fully_state.items():\n",
    "        # 检查权重键是否包含 \"student\"\n",
    "        if 'student' in key:\n",
    "            key = key.replace(\"module.student.\", \"\")\n",
    "            student_weights[key] = value\n",
    "        if 'teacher' in key:\n",
    "            key = key.replace(\"module.teacher.\", \"\")\n",
    "            teacher_weights[key] = value\n",
    "    # model.load_state_dict(load_checkpoint(mpath)[\"model\"])\n",
    "    # tea_model = cifar_model_dict[cfg.DISTILLER.TEACHER][0](num_classes=num_classes)\n",
    "    # tea_model.load_state_dict(load_checkpoint(cifar_model_dict[cfg.DISTILLER.TEACHER][1])[\"model\"])\n",
    "    model.load_state_dict(student_weights)\n",
    "    tea_model = cifar_model_dict[cfg.DISTILLER.TEACHER][0](num_classes=num_classes)\n",
    "    tea_model.load_state_dict(teacher_weights)\n",
    "    print(\"load {} successfully!\".format(mpath))\n",
    "    return model, tea_model, val_loader\n",
    "\n",
    "\n",
    "def fwd(model, val_loader, layer, num_classes=100):\n",
    "    model.eval()\n",
    "    all_preds, all_feats = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in tqdm(enumerate(val_loader)):\n",
    "            if i < 2:\n",
    "                outputs, feats = model(data)\n",
    "                preds = outputs\n",
    "                all_preds.append(preds.data.cpu().numpy())\n",
    "                all_feats.append(feats[\"feats\"][layer].data.cpu().numpy())\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, 0)\n",
    "    all_feats = np.concatenate(all_feats, 0)\n",
    "    return all_preds, all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_path = '/home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/fitnet_324_84/latest'\n",
    "kd_path = '/home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/kd_324_84/latest'\n",
    "dkd_path = '/home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/dkd_324_84/latest'\n",
    "kr_path = '/home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/kr_324_84/latest'\n",
    "our_path = '/home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/UniKD_77.71/latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tea = 'resnet32x4'\n",
    "stu = 'resnet8x4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "load /home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/fitnet_324_84/latest successfully!\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "load /home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/kd_324_84/latest successfully!\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "load /home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/kr_324_84/latest successfully!\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "load /home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/dkd_324_84/latest successfully!\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "load /home/fabien/Documents/project/2d/mdistiller/tools/output/final/324_84/UniKD_77.71/latest successfully!\n"
     ]
    }
   ],
   "source": [
    "fit_stu, fit_tea, _ = load_model(tea, stu, fit_path)\n",
    "kd_stu, kd_tea, _ = load_model(tea, stu, kd_path)\n",
    "kr_stu, kr_tea, _ = load_model(tea, stu, kr_path)\n",
    "dkd_stu, dkd_tea, val_loader = load_model(tea, stu, dkd_path)\n",
    "unikd_stu, unikd_tea, _ = load_model(tea, stu, our_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    " \n",
    "# 解压缩，返回解压后的字典\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def cifar100_to_images():\n",
    "  tar_dir='/home/fabien/Documents/project/2d/mdistiller/data/cifar-100-python/' #原始数据库目录\n",
    "  train_root_dir='./home/fabien/Documents/project/2d/mdistiller/data/cifar_img/train/' #图片保存目录\n",
    "  test_root_dir='/home/fabien/Documents/project/2d/mdistiller/data/cifar_img/test/'\n",
    "  if not os.path.exists(train_root_dir):\n",
    "    os.makedirs(train_root_dir)\n",
    "  if not os.path.exists(test_root_dir):\n",
    "    os.makedirs(test_root_dir)\n",
    " \n",
    "  #获取label对应的class，分为20个coarse class，共100个 fine class\n",
    "  meta_Name = tar_dir+\"meta\" \n",
    "  Meta_dic= unpickle(meta_Name)\n",
    "  coarse_label_names=Meta_dic['coarse_label_names']\n",
    "  fine_label_names=Meta_dic['fine_label_names']\n",
    "  print(fine_label_names)\n",
    " \n",
    "  #生成训练集图片，如果需要png格式，只需要改图片后缀名即可。\n",
    "  dataName = tar_dir+\"train\" \n",
    "  Xtr = unpickle(dataName)\n",
    "  print(dataName + \" is loading...\")\n",
    "  for i in range(0,Xtr['data'].shape[0]):\n",
    "    if i < 100:\n",
    "        img = np.reshape(Xtr['data'][i], (3, 32, 32))  # Xtr['data']为图片二进制数据\n",
    "        img = img.transpose(1, 2, 0)  # 读取image\n",
    "        ###img_name:fine_label+coarse_label+fine_class+coarse_class+index\n",
    "        picName = train_root_dir + str(Xtr['fine_labels'][i])+ '_' + str(Xtr['coarse_labels'][i]) + '_&' +fine_label_names[Xtr['fine_labels'][i]]+'&_'+coarse_label_names[ Xtr['coarse_labels'][i]]+'_'+str(i) + '.png' \n",
    "        cv2.imwrite(picName, img)\n",
    "    else:\n",
    "        break\n",
    "  print(dataName + \" loaded.\")\n",
    " \n",
    "  print(\"test_batch is loading...\")\n",
    "  # 生成测试集图片\n",
    "  testXtr = unpickle(tar_dir+\"test\")\n",
    "  for i in range(0, testXtr['data'].shape[0]):\n",
    "    if i < 100:\n",
    "        img = np.reshape(testXtr['data'][i], (3, 32, 32))\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        picName = test_root_dir +str(testXtr['fine_labels'][i])+ '_' + str(testXtr['coarse_labels'][i]) + '_&' +fine_label_names[testXtr['fine_labels'][i]]+'&_'+coarse_label_names[ testXtr['coarse_labels'][i]]+'_'+str(i) + '.png' \n",
    "        cv2.imwrite(picName, img)\n",
    "    else:\n",
    "       break\n",
    "  print(\"test_batch loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
      "/home/fabien/Documents/project/2d/mdistiller/data/cifar-100-python/train is loading...\n",
      "/home/fabien/Documents/project/2d/mdistiller/data/cifar-100-python/train loaded.\n",
      "test_batch is loading...\n",
      "test_batch loaded.\n"
     ]
    }
   ],
   "source": [
    "cifar100_to_images()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
